[server]
host = "0.0.0.0"
port = 8080
enable_ui = true

[limits]
max_tokens = 1024
max_concurrent = 2
queue_depth = 32

[[models]]
name = "local-llm"
path = "./llama-2-7b-chat.Q4_K_M.gguf"
backend = "llm"
arch = "llama"
device = "cpu"
max_concurrent = 1

[safety]
denylist = ["forbidden_word", "do_not_reply"]
